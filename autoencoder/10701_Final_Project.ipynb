{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "10701 Final Project.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "prDP7lO90pFB",
        "_oPjaAPHVCD0",
        "CjGzE4fMVE7f",
        "7FnS9NrUVKUQ",
        "Xpah5372Khh1",
        "zErjOD43kOo6",
        "wTgj_is_1iuh",
        "5HPEQKYYnF-K",
        "hZwlOTOU0zDX"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prDP7lO90pFB"
      },
      "source": [
        "#Check Hardware Info"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97EqiW4T0nib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5475993-75ea-412a-e1e8-d48d6309aabc"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Dec 16 02:22:01 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.45.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aIUDz5p0nBq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be722a49-c69f-4422-aa8c-29016626d8ba"
      },
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your runtime has 27.4 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oPjaAPHVCD0"
      },
      "source": [
        "#Download Data (Run Only Once)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bG_OoaUxTdvy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c303f8d6-b6e1-4f40-d57c-71722f7e1968"
      },
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle\n",
        "\n",
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"josephzheng1998\"\n",
        "os.environ['KAGGLE_KEY'] = \"72fd2a56fcfde8d42fe037aa8d9be146\"\n",
        "\n",
        "!kaggle datasets download -d msambare/fer2013\n",
        "!unzip -qq fer2013.zip\n",
        "!rm fer2013.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting kaggle\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/33/365c0d13f07a2a54744d027fe20b60dacdfdfb33bc04746db6ad0b79340b/kaggle-1.5.10.tar.gz (59kB)\n",
            "\r\u001b[K     |█████▌                          | 10kB 17.5MB/s eta 0:00:01\r\u001b[K     |███████████                     | 20kB 18.8MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 30kB 10.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 40kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 51kB 4.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 3.0MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: kaggle\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaggle: filename=kaggle-1.5.10-cp36-none-any.whl size=73269 sha256=33a8c4501dba85947f7548a6b6d9bb9047a665315e77c8fc2e361a02b3c3b0d3\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/d1/7e/6ce09b72b770149802c653a02783821629146983ee5a360f10\n",
            "Successfully built kaggle\n",
            "Installing collected packages: kaggle\n",
            "  Found existing installation: kaggle 1.5.10\n",
            "    Uninstalling kaggle-1.5.10:\n",
            "      Successfully uninstalled kaggle-1.5.10\n",
            "Successfully installed kaggle-1.5.10\n",
            "Downloading fer2013.zip to /content\n",
            " 76% 46.0M/60.3M [00:00<00:00, 207MB/s]\n",
            "100% 60.3M/60.3M [00:00<00:00, 257MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kL52QIetnz9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bc6436a-6ba8-4cc9-efe6-420945b80fd6"
      },
      "source": [
        "!pip install split-folders"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting split-folders\n",
            "  Downloading https://files.pythonhosted.org/packages/b8/5f/3c2b2f7ea5e047c8cdc3bb00ae582c5438fcdbbedcc23b3cc1c2c7aae642/split_folders-0.4.3-py3-none-any.whl\n",
            "Installing collected packages: split-folders\n",
            "Successfully installed split-folders-0.4.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nprMpwpJTnMh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b7a57a2-714a-44cc-b06e-f68dfbe00881"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnnEm_0Lt58b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9c24e51-7ca8-4311-e9e1-13a36de80dc8"
      },
      "source": [
        "import splitfolders\n",
        "splitfolders.ratio('test', output='val_test', seed=1337, ratio=(0, 0.5, 0.5), group_prefix=None)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying files: 7178 files [00:01, 6112.94 files/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjGzE4fMVE7f"
      },
      "source": [
        "#Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3Zvk-0WToPy"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import torchvision.models as models\n",
        "from torchvision.datasets import ImageFolder\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from IPython.display import Image\n",
        "from IPython.core.display import Image, display\n",
        "from torchvision.utils import save_image\n",
        "import time\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "%matplotlib inline"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVQ5LmBcTq1_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df22f2d7-11d2-44d1-e952-317dd1626fe1"
      },
      "source": [
        "cuda = torch.cuda.is_available()\n",
        "device = torch.device('cuda' if cuda else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FnS9NrUVKUQ"
      },
      "source": [
        "#Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBrmkY3YURGL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4f04c0c-0500-4639-d49b-0ffd5a83b123"
      },
      "source": [
        "batch_size = 64\n",
        "train_transform = transforms.Compose([\n",
        "                                      transforms.RandomRotation(45),\n",
        "                                      transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "val_transform = transforms.Compose([\n",
        "                                    transforms.ToTensor(),\n",
        "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "train_dataset = ImageFolder(root='train/', transform=train_transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=16, pin_memory=True)\n",
        "\n",
        "dev_dataset = ImageFolder(root='val_test/val/', transform=val_transform)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False, num_workers=16, pin_memory=True)\n",
        "\n",
        "test_dataset = ImageFolder(root='val_test/test/', transform=val_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=16, pin_memory=True)\n",
        "\n",
        "print('train dataset: {} images {} classes'.format(len(train_dataset), len(train_dataset.classes)))\n",
        "print('dev dataset: {} images {} classes'.format(len(dev_dataset), len(dev_dataset.classes)))\n",
        "print('test dataset: {} images {} classes'.format(len(test_dataset), len(test_dataset.classes)))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train dataset: 28709 images 7 classes\n",
            "dev dataset: 3587 images 7 classes\n",
            "test dataset: 3591 images 7 classes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xpah5372Khh1"
      },
      "source": [
        "#Plot Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btz8GbQJJJlJ"
      },
      "source": [
        "def make_plots(a, b, model_name, plot_type):\n",
        "    plt.figure(1)\n",
        "    plt.plot(range(1, len(a) + 1), a, 'b', label='train')\n",
        "    plt.plot(range(1, len(b) + 1), b, 'g', label='valid')\n",
        "    plt.title('{} Train/Valid {}'.format(model_name, plot_type))\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel(plot_type)\n",
        "    plt.legend()\n",
        "    plt.savefig('/content/gdrive/My Drive/{}_{}.png'.format(model_name, '_'.join(plot_type.lower().split())))\n",
        "    plt.show()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reZfLLc8GNFn"
      },
      "source": [
        "def plot_matrix(matrix, dataset, model_name):\n",
        "    emotion_list = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
        "    matrix_df = pd.DataFrame(matrix, index=emotion_list, columns=emotion_list)\n",
        "    plt.figure(figsize=(12,8))\n",
        "    ax = sns.heatmap(matrix_df, cmap='YlGnBu', annot=True, fmt='d', annot_kws={'size': 16}).set_title('{} Confusion Matrix ({})'.format(model_name, dataset))\n",
        "    plt.savefig('/content/gdrive/My Drive/{}_{}_matrix.png'.format('_'.join(model_name.split()), dataset.lower()))\n",
        "    plt.show()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_CZYREgyRmU"
      },
      "source": [
        "def compare(x):\n",
        "    features, idx_list = model.encoder(x)\n",
        "    recon_x = model.decoder(features, idx_list)\n",
        "    return torch.cat([x, recon_x])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zErjOD43kOo6"
      },
      "source": [
        "#Multitasking Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtLDu0hikOEb"
      },
      "source": [
        "class EncoderBlock(nn.Module):\r\n",
        "  def __init__(self, input_chan, output_chan):\r\n",
        "    super(EncoderBlock, self).__init__()\r\n",
        "    self.Conv1 = nn.Conv2d(in_channels=input_chan, out_channels=output_chan, kernel_size=(3,3),stride=(1,1), padding=(1,1))\r\n",
        "    self.norm1 = nn.BatchNorm2d(output_chan)\r\n",
        "    self.act1 = nn.ReLU()\r\n",
        "    self.Conv2 = nn.Conv2d(in_channels=output_chan, out_channels=output_chan, kernel_size=(3,3), stride=(1,1), padding=(1,1))\r\n",
        "    self.norm2 = nn.BatchNorm2d(output_chan)\r\n",
        "    self.act2 = nn.ReLU()\r\n",
        "    self.pool = nn.MaxPool2d(kernel_size=(2,2),stride=(2,2), return_indices=True)\r\n",
        "  def forward(self,x):\r\n",
        "    x=self.Conv1(x)\r\n",
        "    x=self.norm1(x)\r\n",
        "    x=self.act1(x)\r\n",
        "    x=self.Conv2(x)\r\n",
        "    x=self.norm2(x)\r\n",
        "    x=self.act2(x)\r\n",
        "    x,indices = self.pool(x)\r\n",
        "    return (x, indices)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "class Encoder(nn.Module):\r\n",
        "  def __init__(self):\r\n",
        "    super(Encoder, self).__init__()\r\n",
        "    self.encoderLayer1 = EncoderBlock(3,32)\r\n",
        "    self.encoderLayer2 = EncoderBlock(32,64)\r\n",
        "    self.encoderLayer3 = EncoderBlock(64,128)\r\n",
        "    self.encoderLayer4 = EncoderBlock(128,256)\r\n",
        "    self.encoderLin = nn.Sequential(\r\n",
        "        nn.Flatten(),\r\n",
        "        #change output to 686\r\n",
        "        nn.Linear(256*3*3, 686)\r\n",
        "    )\r\n",
        "  def forward(self,x):\r\n",
        "    indicesList = []\r\n",
        "    x,indices1 = self.encoderLayer1(x)\r\n",
        "    indicesList.append(indices1)\r\n",
        "    x,indices2 = self.encoderLayer2(x)\r\n",
        "    indicesList.append(indices2)\r\n",
        "    x,indices3 = self.encoderLayer3(x)\r\n",
        "    indicesList.append(indices3)\r\n",
        "    x,indices4 = self.encoderLayer4(x)\r\n",
        "    indicesList.append(indices4)\r\n",
        "    x=self.encoderLin(x)\r\n",
        "    return x, indicesList\r\n",
        "\r\n",
        "class Decoder(nn.Module):\r\n",
        "  def __init__(self):\r\n",
        "    super(Decoder, self).__init__()\r\n",
        "    self.TConv1 = nn.ConvTranspose2d(in_channels=256, out_channels=256,kernel_size=(3,3), stride=(1,1), padding=(1,1))\r\n",
        "    self.norm1 = nn.BatchNorm2d(256)\r\n",
        "    self.act1 = nn.ReLU()\r\n",
        "    self.pool1 = nn.MaxUnpool2d(kernel_size=(2,2),stride=(2,2))\r\n",
        "    self.TConv2 = nn.ConvTranspose2d(in_channels=256, out_channels=128,kernel_size=(3,3), stride=(1,1), padding=(1,1))\r\n",
        "    self.norm2 = nn.BatchNorm2d(128)\r\n",
        "    self.act2 = nn.ReLU()\r\n",
        "          \r\n",
        "    self.pool2 = nn.MaxUnpool2d(kernel_size=(2,2),stride=(2,2))\r\n",
        "    self.TConv3 = nn.ConvTranspose2d(in_channels=128, out_channels=128,kernel_size=(3,3), stride=(1,1), padding=(1,1))\r\n",
        "    self.norm3 = nn.BatchNorm2d(128)\r\n",
        "    self.act3 = nn.ReLU()\r\n",
        "    self.TConv4 = nn.ConvTranspose2d(in_channels=128, out_channels=64,kernel_size=(3,3), stride=(1,1), padding=(1,1))\r\n",
        "    self.norm4 = nn.BatchNorm2d(64)\r\n",
        "    self.act4 = nn.ReLU()\r\n",
        "           \r\n",
        "    self.pool3 = nn.MaxUnpool2d(kernel_size=(2,2),stride=(2,2))\r\n",
        "    self.TConv5 = nn.ConvTranspose2d(in_channels=64, out_channels=64,kernel_size=(3,3), stride=(1,1), padding=(1,1))\r\n",
        "    self.norm5 = nn.BatchNorm2d(64)\r\n",
        "    self.act5 = nn.ReLU()\r\n",
        "    self.TConv6 = nn.ConvTranspose2d(in_channels=64, out_channels=32,kernel_size=(3,3), stride=(1,1), padding=(1,1))\r\n",
        "    self.norm6 = nn.BatchNorm2d(32)\r\n",
        "    self.act6 = nn.ReLU()\r\n",
        "          \r\n",
        "    self.pool4 = nn.MaxUnpool2d(kernel_size=(2,2),stride=(2,2))\r\n",
        "    self.TConv7 = nn.ConvTranspose2d(in_channels=32, out_channels=32,kernel_size=(3,3), stride=(1,1), padding=(1,1))\r\n",
        "    self.norm7 = nn.BatchNorm2d(32)\r\n",
        "    self.act7 = nn.ReLU()\r\n",
        "    self.TConv8 = nn.ConvTranspose2d(in_channels=32, out_channels=3,kernel_size=(3,3), stride=(1,1), padding=(1,1))\r\n",
        "    self.decoderLin = nn.Sequential(\r\n",
        "        #change input to 686\r\n",
        "        nn.Linear(686, 256*3*3)\r\n",
        "    )\r\n",
        "  def forward(self,x,indicesList):\r\n",
        "    x=self.decoderLin(x)\r\n",
        "    x=x.view(-1,256,3,3)\r\n",
        "    x=self.TConv1(x)\r\n",
        "    x=self.norm1(x)\r\n",
        "    x=self.act1(x)\r\n",
        "    x=self.pool1(x, indicesList[3])\r\n",
        "    x=self.TConv2(x)\r\n",
        "    x=self.norm2(x)\r\n",
        "    x=self.act2(x)\r\n",
        "\r\n",
        "    x=self.pool2(x, indicesList[2])\r\n",
        "    x=self.TConv3(x)\r\n",
        "    x=self.norm3(x)\r\n",
        "    x=self.act3(x)\r\n",
        "    x=self.TConv4(x)\r\n",
        "    x=self.norm4(x)\r\n",
        "    x=self.act4(x)\r\n",
        "\r\n",
        "    x=self.pool3(x, indicesList[1])\r\n",
        "    x=self.TConv5(x)\r\n",
        "    x=self.norm5(x)\r\n",
        "    x=self.act5(x)\r\n",
        "    x=self.TConv6(x)\r\n",
        "    x=self.norm6(x)\r\n",
        "    x=self.act6(x)\r\n",
        "\r\n",
        "    x=self.pool4(x, indicesList[0])\r\n",
        "    x=self.TConv7(x)\r\n",
        "    x=self.norm7(x)\r\n",
        "    x=self.act7(x)\r\n",
        "    x=self.TConv8(x)\r\n",
        "    return x\r\n",
        "\r\n",
        "class LinClassifier(nn.Module):\r\n",
        "  def __init__(self):\r\n",
        "    super(LinClassifier, self).__init__()\r\n",
        "    self.layers = nn.Sequential(\r\n",
        "        nn.ReLU(),\r\n",
        "        #change input to 686\r\n",
        "        nn.Linear(686,512),\r\n",
        "        nn.ReLU(),\r\n",
        "        nn.Dropout(0.5),\r\n",
        "        nn.Linear(512,7)\r\n",
        "    )\r\n",
        "  def forward(self, x):\r\n",
        "    return self.layers(x) \r\n",
        "\r\n",
        "\r\n",
        "class AutoClassifier(nn.Module):\r\n",
        "  def __init__(self):\r\n",
        "    super(AutoClassifier, self).__init__()\r\n",
        "    self.encoder = Encoder()\r\n",
        "    self.decoder = Decoder()\r\n",
        "    self.classifier = LinClassifier()\r\n",
        "  def forwardAuto(self,x):\r\n",
        "    x,indicesList=self.encoder(x)\r\n",
        "    reconstructed=self.classifier(x, indicesList)\r\n",
        "    return x\r\n",
        "  def forwardClassify(self,x):\r\n",
        "    x,indicesList = self.encoder(x)\r\n",
        "    output = self.classifier(x)\r\n",
        "    return x\r\n",
        "\r\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0yR4lQfLFFj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgsWZWzdkZMG"
      },
      "source": [
        "# train the multitasking model\n",
        "def train(model, train_loader, val_loader, recon_criterion, class_criterion, optimizer, scheduler, epochs, device):\n",
        "    t_start = time.time()\n",
        "    train_recon_losses = []\n",
        "    train_class_losses = []\n",
        "    train_accs = []\n",
        "    val_class_losses = []\n",
        "    val_recon_losses = []\n",
        "    val_accs = []\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_recon_loss = 0\n",
        "        running_class_loss = 0\n",
        "        total = 0\n",
        "        correct = 0\n",
        "        for idx, (images, labels) in enumerate(train_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            features, idx_list = model.encoder.forward(images)\n",
        "            output = model.classifier.forward(features)\n",
        "            recon_images = model.decoder.forward(features, idx_list)\n",
        "            class_loss = class_criterion(output, labels)\n",
        "            recon_loss = recon_criterion(recon_images, images)\n",
        "            total_loss = class_loss + 10*recon_loss \n",
        "            total_loss.backward()\n",
        "            #class_loss.backward()\n",
        "            optimizer.step()\n",
        "            preds = torch.argmax(output, dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            running_recon_loss += float(recon_loss.item() * images.shape[0])\n",
        "            running_class_loss += float(class_loss.item() * images.shape[0])\n",
        "            total += images.shape[0]\n",
        "            #recon_loss = 0\n",
        "            del images\n",
        "            del labels\n",
        "            del recon_images\n",
        "            del features\n",
        "            del idx_list\n",
        "            del output\n",
        "            del recon_loss\n",
        "            del class_loss\n",
        "            del preds\n",
        "            del total_loss\n",
        "            torch.cuda.empty_cache()\n",
        "        val_class_loss, val_recon_loss, val_acc = validate(model, val_loader, recon_criterion, class_criterion, device)\n",
        "        val_total_loss = val_recon_loss + val_class_loss\n",
        "        train_recon_loss = running_recon_loss/total\n",
        "        train_class_loss = running_class_loss/total\n",
        "        train_total_loss = train_recon_loss + train_class_loss\n",
        "        train_acc = correct/total \n",
        "        train_recon_losses.append(train_recon_loss)\n",
        "        train_class_losses.append(train_class_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_class_losses.append(val_class_loss)\n",
        "        val_recon_losses.append(val_recon_loss)\n",
        "        val_accs.append(val_acc)\n",
        "        scheduler.step(val_class_loss)\n",
        "        to_print = \"Epoch: {}/{}, Training Time:{:.2f}, Trained Samples: {}/{}, Train Total Loss: {:.5f}, Train Recon Loss: {:.5f}, Train Class Loss: {:.5f} Train Accuracy: {:.5f}, Val Total Loss: {:.5f}, Val Recon Loss: {:.5f}, Val Class Loss: {:.5f}, Val Accuracy: {:.5f}\".format(\n",
        "                epoch+1, epochs, time.time()-t_start, total, len(train_loader.dataset), train_total_loss, train_recon_loss, train_class_loss, train_acc,\n",
        "                    val_total_loss, val_recon_loss, val_class_loss, val_acc)\n",
        "        print(to_print)\n",
        "        if (epoch+1) % 10 == 0:\n",
        "            saved_model = {\n",
        "                        'train_epochs': epoch + 1,\n",
        "                        'model': model.state_dict(),\n",
        "                        'optimizer': optimizer.state_dict(),\n",
        "                        'recon_criterion': recon_criterion.state_dict(),\n",
        "                        'class_criterion': class_criterion.state_dict(),\n",
        "                        'scheduler': scheduler.state_dict(),\n",
        "                        'train_class_losses': train_class_losses,\n",
        "                        'train_recon_losses': train_recon_losses,\n",
        "                        'train_accuracy': train_accs,\n",
        "                        'valid_class_losses': val_class_losses,\n",
        "                        'valid_accuracy': val_accs,\n",
        "                        'val_recon_losses': val_recon_losses,\n",
        "                        }\n",
        "            torch.save(saved_model, 'gdrive/MyDrive/complete_model{}'.format(epoch+1))\n",
        "    return train_recon_losses, train_class_losses, train_accs, val_class_losses, val_recon_losses, val_accs\n",
        "\n",
        "# validate the multitasking model\n",
        "def validate(model, val_loader, recon_criterion, class_criterion, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    running_recon_loss = 0\n",
        "    running_class_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for idx, (images, labels) in enumerate(val_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            features, idx_list = model.encoder.forward(images)\n",
        "            output = model.classifier.forward(features)\n",
        "            recon_images = model.decoder.forward(features, idx_list)\n",
        "            class_loss = class_criterion(output, labels)\n",
        "            recon_loss = recon_criterion(recon_images, images)\n",
        "            running_recon_loss += float(recon_loss.item() * labels.shape[0])\n",
        "            running_class_loss += float(class_loss * labels.shape[0])\n",
        "            total += labels.shape[0]\n",
        "            preds = torch.argmax(output, dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            \"\"\"\n",
        "            print(correct)\n",
        "            print(preds)\n",
        "            print(labels)\n",
        "            \"\"\"\n",
        "            del images\n",
        "            del labels\n",
        "            del recon_images\n",
        "            del features\n",
        "            del idx_list\n",
        "            del output\n",
        "            del recon_loss\n",
        "            del class_loss\n",
        "            del preds\n",
        "            torch.cuda.empty_cache()\n",
        "    return running_class_loss/total, running_recon_loss/total, correct/total\n",
        "\n",
        "\n",
        "# only train the autoencoder part of the model\n",
        "def pretrain(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs, device):\n",
        "    t_start = time.time()\n",
        "    train_recon_losses = []\n",
        "    val_recon_losses = []\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_recon_loss = 0\n",
        "        total = 0\n",
        "        for idx, (images, labels) in enumerate(train_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            features, idx_list = model.encoder.forward(images)\n",
        "            recon_images = model.decoder.forward(features, idx_list)\n",
        "            recon_loss = criterion(recon_images, images)\n",
        "            recon_loss.backward()\n",
        "            optimizer.step()\n",
        "            running_recon_loss += float(recon_loss.item() * images.shape[0])\n",
        "            total += images.shape[0]\n",
        "            del images\n",
        "            del labels\n",
        "            del recon_images\n",
        "            del features\n",
        "            del idx_list\n",
        "            del recon_loss\n",
        "            torch.cuda.empty_cache()\n",
        "        val_recon_loss = pre_validate(model, val_loader, criterion, device)\n",
        "        train_recon_loss = running_recon_loss/total \n",
        "        train_recon_losses.append(train_recon_loss)\n",
        "        val_recon_losses.append(val_recon_loss)\n",
        "        scheduler.step(val_recon_loss)\n",
        "        to_print = \"Epoch: {}/{}, Training Time:{:.2f}, Trained Samples: {}/{}, Train Recon Loss: {:.5f}, Val Recon Loss: {:.5f}\".format(\n",
        "                epoch+1, epochs, time.time()-t_start, total, len(train_loader.dataset), train_recon_loss,\n",
        "                     val_recon_loss)\n",
        "        print(to_print)\n",
        "        if epoch % 10 == 0:\n",
        "            saved_model = {\n",
        "                        'train_epochs': epoch + 1,\n",
        "                        'model': model.state_dict(),\n",
        "                        'optimizer': optimizer.state_dict(),\n",
        "                        'recon_criterion': criterion.state_dict(),\n",
        "                        'scheduler': scheduler.state_dict(),\n",
        "                        }\n",
        "            torch.save(saved_model, 'gdrive/MyDrive/pre_model{}'.format(epoch))\n",
        "    return train_recon_losses, val_recon_losses \n",
        "\n",
        "# only validate the autoencoder part of the model\n",
        "def pre_validate(model, val_loader, recon_criterion, device):\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    running_recon_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for idx, (images, labels) in enumerate(val_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            features, idx_list = model.encoder.forward(images)\n",
        "            recon_images = model.decoder.forward(features, idx_list)\n",
        "            recon_loss = recon_criterion(recon_images, images)\n",
        "            running_recon_loss += float(recon_loss.item() * labels.shape[0])\n",
        "            total += images.shape[0]\n",
        "            del images\n",
        "            del labels\n",
        "            del recon_images\n",
        "            del features\n",
        "            del idx_list\n",
        "            del recon_loss\n",
        "            torch.cuda.empty_cache()\n",
        "    return running_recon_loss/total\n",
        "\n",
        "# only train the model's classifier\n",
        "def train_class(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs, device):\n",
        "    t_start = time.time()\n",
        "    train_class_losses = []\n",
        "    train_accs = []\n",
        "    val_class_losses = []\n",
        "    val_accs = []\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_class_loss = 0\n",
        "        total = 0\n",
        "        correct = 0\n",
        "        for idx, (images, labels) in enumerate(train_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            features, idx_list = model.encoder.forward(images)\n",
        "            features = features.detach()\n",
        "            output = model.classifier.forward(features)\n",
        "            class_loss = criterion(output, labels)\n",
        "            class_loss.backward()\n",
        "            optimizer.step()\n",
        "            preds = torch.argmax(output, dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            running_class_loss += float(class_loss.item() * images.shape[0])\n",
        "            total += images.shape[0]\n",
        "            del images\n",
        "            del labels\n",
        "            del features\n",
        "            del idx_list\n",
        "            del output\n",
        "            del class_loss\n",
        "            del preds\n",
        "            torch.cuda.empty_cache()\n",
        "        val_class_loss, val_acc = validate_class(model, val_loader, criterion, device)\n",
        "        train_class_loss = running_class_loss/total\n",
        "        train_acc = correct/total \n",
        "        train_class_losses.append(train_class_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_class_losses.append(val_class_loss)\n",
        "        val_accs.append(val_acc)\n",
        "        scheduler.step(val_class_loss)\n",
        "        to_print = \"Epoch: {}/{}, Training Time:{:.2f}, Trained Samples: {}/{}, Train Class Loss: {:.5f} Train Accuracy: {:.5f}, Val Class Loss: {:.5f}, Val Accuracy: {:.5f}\".format(\n",
        "                epoch+1, epochs, time.time()-t_start, total, len(train_loader.dataset), train_class_loss, train_acc,\n",
        "                     val_class_loss, val_acc)\n",
        "        print(to_print)\n",
        "        if (epoch+1) % 10 == 0:\n",
        "            saved_model = {\n",
        "                        'train_epochs': epoch + 1,\n",
        "                        'model': model.state_dict(),\n",
        "                        'optimizer': optimizer.state_dict(),\n",
        "                        'criterion': criterion.state_dict(),\n",
        "                        'scheduler': scheduler.state_dict(),\n",
        "                        'train_class_losses': train_class_losses,\n",
        "                        'train_accuracy': train_accs,\n",
        "                        'valid_class_losses': val_class_losses,\n",
        "                        'valid_accuracy': val_accs,\n",
        "                        }\n",
        "            torch.save(saved_model, 'gdrive/MyDrive/complete_model{}'.format(epoch+1))\n",
        "    return train_class_losses, train_accs, val_class_losses, val_accs\n",
        "\n",
        "# only validate the model's classifier\n",
        "def validate_class(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    running_class_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for idx, (images, labels) in enumerate(val_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            features, idx_list = model.encoder.forward(images)\n",
        "            features = features.detach()\n",
        "            output = model.classifier.forward(features)\n",
        "            class_loss = class_criterion(output, labels)\n",
        "            running_class_loss += float(class_loss * labels.shape[0])\n",
        "            total += labels.shape[0]\n",
        "            preds = torch.argmax(output, dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            \"\"\"\n",
        "            print(correct)\n",
        "            print(preds)\n",
        "            print(labels)\n",
        "            \"\"\"\n",
        "            del images\n",
        "            del labels\n",
        "            del features\n",
        "            del idx_list\n",
        "            del output\n",
        "            del class_loss\n",
        "            del preds\n",
        "            torch.cuda.empty_cache()\n",
        "    return running_class_loss/total, correct/total"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTgj_is_1iuh"
      },
      "source": [
        "# Training and Validating Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkH_k41_1mL2"
      },
      "source": [
        "# train the multitasking model\n",
        "def train(model, train_loader, val_loader, recon_criterion, class_criterion, optimizer, scheduler, epochs, device):\n",
        "    t_start = time.time()\n",
        "    train_recon_losses = []\n",
        "    train_class_losses = []\n",
        "    train_accs = []\n",
        "    val_class_losses = []\n",
        "    val_recon_losses = []\n",
        "    val_accs = []\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_recon_loss = 0\n",
        "        running_class_loss = 0\n",
        "        total = 0\n",
        "        correct = 0\n",
        "        for idx, (images, labels) in enumerate(train_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            features, idx_list = model.encoder.forward(images)\n",
        "            output = model.classifier.forward(features)\n",
        "            recon_images = model.decoder.forward(features, idx_list)\n",
        "            class_loss = class_criterion(output, labels)\n",
        "            recon_loss = recon_criterion(recon_images, images)\n",
        "            total_loss = class_loss + 10*recon_loss \n",
        "            total_loss.backward()\n",
        "            #class_loss.backward()\n",
        "            optimizer.step()\n",
        "            preds = torch.argmax(output, dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            running_recon_loss += float(recon_loss.item() * images.shape[0])\n",
        "            running_class_loss += float(class_loss.item() * images.shape[0])\n",
        "            total += images.shape[0]\n",
        "            #recon_loss = 0\n",
        "            del images\n",
        "            del labels\n",
        "            del recon_images\n",
        "            del features\n",
        "            del idx_list\n",
        "            del output\n",
        "            del recon_loss\n",
        "            del class_loss\n",
        "            del preds\n",
        "            del total_loss\n",
        "            torch.cuda.empty_cache()\n",
        "        val_class_loss, val_recon_loss, val_acc = validate(model, val_loader, recon_criterion, class_criterion, device)\n",
        "        val_total_loss = val_recon_loss + val_class_loss\n",
        "        train_recon_loss = running_recon_loss/total\n",
        "        train_class_loss = running_class_loss/total\n",
        "        train_total_loss = train_recon_loss + train_class_loss\n",
        "        train_acc = correct/total \n",
        "        train_recon_losses.append(train_recon_loss)\n",
        "        train_class_losses.append(train_class_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_class_losses.append(val_class_loss)\n",
        "        val_recon_losses.append(val_recon_loss)\n",
        "        val_accs.append(val_acc)\n",
        "        scheduler.step(val_class_loss)\n",
        "        to_print = \"Epoch: {}/{}, Training Time:{:.2f}, Trained Samples: {}/{}, Train Total Loss: {:.5f}, Train Recon Loss: {:.5f}, Train Class Loss: {:.5f} Train Accuracy: {:.5f}, Val Total Loss: {:.5f}, Val Recon Loss: {:.5f}, Val Class Loss: {:.5f}, Val Accuracy: {:.5f}\".format(\n",
        "                epoch+1, epochs, time.time()-t_start, total, len(train_loader.dataset), train_total_loss, train_recon_loss, train_class_loss, train_acc,\n",
        "                    val_total_loss, val_recon_loss, val_class_loss, val_acc)\n",
        "        print(to_print)\n",
        "        if (epoch+1) % 10 == 0:\n",
        "            saved_model = {\n",
        "                        'train_epochs': epoch + 1,\n",
        "                        'model': model.state_dict(),\n",
        "                        'optimizer': optimizer.state_dict(),\n",
        "                        'recon_criterion': recon_criterion.state_dict(),\n",
        "                        'class_criterion': class_criterion.state_dict(),\n",
        "                        'scheduler': scheduler.state_dict(),\n",
        "                        'train_class_losses': train_class_losses,\n",
        "                        'train_recon_losses': train_recon_losses,\n",
        "                        'train_accuracy': train_accs,\n",
        "                        'valid_class_losses': val_class_losses,\n",
        "                        'valid_accuracy': val_accs,\n",
        "                        'val_recon_losses': val_recon_losses,\n",
        "                        }\n",
        "            torch.save(saved_model, 'gdrive/MyDrive/complete_model{}'.format(epoch+1))\n",
        "    return train_recon_losses, train_class_losses, train_accs, val_class_losses, val_recon_losses, val_accs\n",
        "\n",
        "# validate the multitasking model\n",
        "def validate(model, val_loader, recon_criterion, class_criterion, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    running_recon_loss = 0\n",
        "    running_class_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for idx, (images, labels) in enumerate(val_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            features, idx_list = model.encoder.forward(images)\n",
        "            output = model.classifier.forward(features)\n",
        "            recon_images = model.decoder.forward(features, idx_list)\n",
        "            class_loss = class_criterion(output, labels)\n",
        "            recon_loss = recon_criterion(recon_images, images)\n",
        "            running_recon_loss += float(recon_loss.item() * labels.shape[0])\n",
        "            running_class_loss += float(class_loss * labels.shape[0])\n",
        "            total += labels.shape[0]\n",
        "            preds = torch.argmax(output, dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            \"\"\"\n",
        "            print(correct)\n",
        "            print(preds)\n",
        "            print(labels)\n",
        "            \"\"\"\n",
        "            del images\n",
        "            del labels\n",
        "            del recon_images\n",
        "            del features\n",
        "            del idx_list\n",
        "            del output\n",
        "            del recon_loss\n",
        "            del class_loss\n",
        "            del preds\n",
        "            torch.cuda.empty_cache()\n",
        "    return running_class_loss/total, running_recon_loss/total, correct/total\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oQj-nZ01s4p"
      },
      "source": [
        "# only train the autoencoder part of the model\n",
        "def pretrain(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs, device):\n",
        "    t_start = time.time()\n",
        "    train_recon_losses = []\n",
        "    val_recon_losses = []\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_recon_loss = 0\n",
        "        total = 0\n",
        "        for idx, (images, labels) in enumerate(train_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            features, idx_list = model.encoder.forward(images)\n",
        "            recon_images = model.decoder.forward(features, idx_list)\n",
        "            recon_loss = criterion(recon_images, images)\n",
        "            recon_loss.backward()\n",
        "            optimizer.step()\n",
        "            running_recon_loss += float(recon_loss.item() * images.shape[0])\n",
        "            total += images.shape[0]\n",
        "            del images\n",
        "            del labels\n",
        "            del recon_images\n",
        "            del features\n",
        "            del idx_list\n",
        "            del recon_loss\n",
        "            torch.cuda.empty_cache()\n",
        "        val_recon_loss = pre_validate(model, val_loader, criterion, device)\n",
        "        train_recon_loss = running_recon_loss/total \n",
        "        train_recon_losses.append(train_recon_loss)\n",
        "        val_recon_losses.append(val_recon_loss)\n",
        "        scheduler.step(val_recon_loss)\n",
        "        to_print = \"Epoch: {}/{}, Training Time:{:.2f}, Trained Samples: {}/{}, Train Recon Loss: {:.5f}, Val Recon Loss: {:.5f}\".format(\n",
        "                epoch+1, epochs, time.time()-t_start, total, len(train_loader.dataset), train_recon_loss,\n",
        "                     val_recon_loss)\n",
        "        print(to_print)\n",
        "        if epoch % 10 == 0:\n",
        "            saved_model = {\n",
        "                        'train_epochs': epoch + 1,\n",
        "                        'model': model.state_dict(),\n",
        "                        'optimizer': optimizer.state_dict(),\n",
        "                        'recon_criterion': criterion.state_dict(),\n",
        "                        'scheduler': scheduler.state_dict(),\n",
        "                        }\n",
        "            torch.save(saved_model, 'gdrive/MyDrive/pre_model{}'.format(epoch))\n",
        "    return train_recon_losses, val_recon_losses \n",
        "\n",
        "# only validate the autoencoder part of the model\n",
        "def pre_validate(model, val_loader, recon_criterion, device):\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    running_recon_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for idx, (images, labels) in enumerate(val_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            features, idx_list = model.encoder.forward(images)\n",
        "            recon_images = model.decoder.forward(features, idx_list)\n",
        "            recon_loss = recon_criterion(recon_images, images)\n",
        "            running_recon_loss += float(recon_loss.item() * labels.shape[0])\n",
        "            total += images.shape[0]\n",
        "            del images\n",
        "            del labels\n",
        "            del recon_images\n",
        "            del features\n",
        "            del idx_list\n",
        "            del recon_loss\n",
        "            torch.cuda.empty_cache()\n",
        "    return running_recon_loss/total\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWIbQtAi1wzQ"
      },
      "source": [
        "# only train the model's classifier\n",
        "def train_class(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs, device):\n",
        "    t_start = time.time()\n",
        "    train_class_losses = []\n",
        "    train_accs = []\n",
        "    val_class_losses = []\n",
        "    val_accs = []\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_class_loss = 0\n",
        "        total = 0\n",
        "        correct = 0\n",
        "        for idx, (images, labels) in enumerate(train_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            features, idx_list = model.encoder.forward(images)\n",
        "            features = features.detach()\n",
        "            output = model.classifier.forward(features)\n",
        "            class_loss = criterion(output, labels)\n",
        "            class_loss.backward()\n",
        "            optimizer.step()\n",
        "            preds = torch.argmax(output, dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            running_class_loss += float(class_loss.item() * images.shape[0])\n",
        "            total += images.shape[0]\n",
        "            del images\n",
        "            del labels\n",
        "            del features\n",
        "            del idx_list\n",
        "            del output\n",
        "            del class_loss\n",
        "            del preds\n",
        "            torch.cuda.empty_cache()\n",
        "        val_class_loss, val_acc = validate_class(model, val_loader, criterion, device)\n",
        "        train_class_loss = running_class_loss/total\n",
        "        train_acc = correct/total \n",
        "        train_class_losses.append(train_class_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_class_losses.append(val_class_loss)\n",
        "        val_accs.append(val_acc)\n",
        "        scheduler.step(val_class_loss)\n",
        "        to_print = \"Epoch: {}/{}, Training Time:{:.2f}, Trained Samples: {}/{}, Train Class Loss: {:.5f} Train Accuracy: {:.5f}, Val Class Loss: {:.5f}, Val Accuracy: {:.5f}\".format(\n",
        "                epoch+1, epochs, time.time()-t_start, total, len(train_loader.dataset), train_class_loss, train_acc,\n",
        "                     val_class_loss, val_acc)\n",
        "        print(to_print)\n",
        "        if (epoch+1) % 10 == 0:\n",
        "            saved_model = {\n",
        "                        'train_epochs': epoch + 1,\n",
        "                        'model': model.state_dict(),\n",
        "                        'optimizer': optimizer.state_dict(),\n",
        "                        'criterion': criterion.state_dict(),\n",
        "                        'scheduler': scheduler.state_dict(),\n",
        "                        'train_class_losses': train_class_losses,\n",
        "                        'train_accuracy': train_accs,\n",
        "                        'valid_class_losses': val_class_losses,\n",
        "                        'valid_accuracy': val_accs,\n",
        "                        }\n",
        "            torch.save(saved_model, 'gdrive/MyDrive/complete_model{}'.format(epoch+1))\n",
        "    return train_class_losses, train_accs, val_class_losses, val_accs\n",
        "\n",
        "# only validate the model's classifier\n",
        "def validate_class(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    running_class_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for idx, (images, labels) in enumerate(val_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            features, idx_list = model.encoder.forward(images)\n",
        "            features = features.detach()\n",
        "            output = model.classifier.forward(features)\n",
        "            class_loss = class_criterion(output, labels)\n",
        "            running_class_loss += float(class_loss * labels.shape[0])\n",
        "            total += labels.shape[0]\n",
        "            preds = torch.argmax(output, dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            \"\"\"\n",
        "            print(correct)\n",
        "            print(preds)\n",
        "            print(labels)\n",
        "            \"\"\"\n",
        "            del images\n",
        "            del labels\n",
        "            del features\n",
        "            del idx_list\n",
        "            del output\n",
        "            del class_loss\n",
        "            del preds\n",
        "            torch.cuda.empty_cache()\n",
        "    return running_class_loss/total, correct/total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HPEQKYYnF-K"
      },
      "source": [
        "# Dimensionality Reduction "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EHOFog9nLqU",
        "outputId": "ce52c487-b355-4d9e-8301-ee6e5df7c783"
      },
      "source": [
        "model = AutoClassifier()\n",
        "model.to(device)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AutoClassifier(\n",
              "  (encoder): Encoder(\n",
              "    (encoderLayer1): EncoderBlock(\n",
              "      (Conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act1): ReLU()\n",
              "      (Conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act2): ReLU()\n",
              "      (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "    (encoderLayer2): EncoderBlock(\n",
              "      (Conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act1): ReLU()\n",
              "      (Conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act2): ReLU()\n",
              "      (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "    (encoderLayer3): EncoderBlock(\n",
              "      (Conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act1): ReLU()\n",
              "      (Conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act2): ReLU()\n",
              "      (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "    (encoderLayer4): EncoderBlock(\n",
              "      (Conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act1): ReLU()\n",
              "      (Conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act2): ReLU()\n",
              "      (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "    (encoderLin): Sequential(\n",
              "      (0): Flatten(start_dim=1, end_dim=-1)\n",
              "      (1): Linear(in_features=2304, out_features=1000, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (TConv1): ConvTranspose2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (act1): ReLU()\n",
              "    (pool1): MaxUnpool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
              "    (TConv2): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (act2): ReLU()\n",
              "    (pool2): MaxUnpool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
              "    (TConv3): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (act3): ReLU()\n",
              "    (TConv4): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (act4): ReLU()\n",
              "    (pool3): MaxUnpool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
              "    (TConv5): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (act5): ReLU()\n",
              "    (TConv6): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm6): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (act6): ReLU()\n",
              "    (pool4): MaxUnpool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
              "    (TConv7): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (act7): ReLU()\n",
              "    (TConv8): ConvTranspose2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (decoderLin): Sequential(\n",
              "      (0): Linear(in_features=1000, out_features=2304, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (classifier): LinClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): ReLU()\n",
              "      (1): Linear(in_features=1000, out_features=512, bias=True)\n",
              "      (2): ReLU()\n",
              "      (3): Dropout(p=0.5, inplace=False)\n",
              "      (4): Linear(in_features=512, out_features=7, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGg9t6agoMMm"
      },
      "source": [
        "criterion = nn.MSELoss()\n",
        "epochs = 80\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-2)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.75, patience=2, verbose=True)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L167EdZFoRSb",
        "outputId": "dd47807c-79cc-4875-a3e5-dc5c6f4ed950"
      },
      "source": [
        "# train the encoder and decoder\n",
        "pre_train_loss, pre_val_loss = pretrain(model, train_loader, dev_loader, criterion, optimizer, scheduler, epochs, device)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/80, Training Time:28.36, Trained Samples: 28709/28709, Train Recon Loss: 0.84157, Val Recon Loss: 0.21435\n",
            "Epoch: 2/80, Training Time:59.12, Trained Samples: 28709/28709, Train Recon Loss: 0.19319, Val Recon Loss: 0.14209\n",
            "Epoch: 3/80, Training Time:87.07, Trained Samples: 28709/28709, Train Recon Loss: 0.14194, Val Recon Loss: 0.12963\n",
            "Epoch: 4/80, Training Time:115.10, Trained Samples: 28709/28709, Train Recon Loss: 0.11730, Val Recon Loss: 0.08492\n",
            "Epoch: 5/80, Training Time:142.90, Trained Samples: 28709/28709, Train Recon Loss: 0.10268, Val Recon Loss: 0.08074\n",
            "Epoch: 6/80, Training Time:170.62, Trained Samples: 28709/28709, Train Recon Loss: 0.09202, Val Recon Loss: 0.09384\n",
            "Epoch: 7/80, Training Time:198.51, Trained Samples: 28709/28709, Train Recon Loss: 0.08590, Val Recon Loss: 0.06385\n",
            "Epoch: 8/80, Training Time:226.19, Trained Samples: 28709/28709, Train Recon Loss: 0.08277, Val Recon Loss: 0.06154\n",
            "Epoch: 9/80, Training Time:253.81, Trained Samples: 28709/28709, Train Recon Loss: 0.07928, Val Recon Loss: 0.05822\n",
            "Epoch: 10/80, Training Time:281.53, Trained Samples: 28709/28709, Train Recon Loss: 0.07583, Val Recon Loss: 0.05959\n",
            "Epoch: 11/80, Training Time:309.20, Trained Samples: 28709/28709, Train Recon Loss: 0.07419, Val Recon Loss: 0.09978\n",
            "Epoch    12: reducing learning rate of group 0 to 3.7500e-02.\n",
            "Epoch: 12/80, Training Time:340.16, Trained Samples: 28709/28709, Train Recon Loss: 0.07098, Val Recon Loss: 0.06118\n",
            "Epoch: 13/80, Training Time:367.86, Trained Samples: 28709/28709, Train Recon Loss: 0.06668, Val Recon Loss: 0.05199\n",
            "Epoch: 14/80, Training Time:395.64, Trained Samples: 28709/28709, Train Recon Loss: 0.06677, Val Recon Loss: 0.05277\n",
            "Epoch: 15/80, Training Time:423.64, Trained Samples: 28709/28709, Train Recon Loss: 0.06619, Val Recon Loss: 0.05384\n",
            "Epoch    16: reducing learning rate of group 0 to 2.8125e-02.\n",
            "Epoch: 16/80, Training Time:451.47, Trained Samples: 28709/28709, Train Recon Loss: 0.06343, Val Recon Loss: 0.05501\n",
            "Epoch: 17/80, Training Time:479.24, Trained Samples: 28709/28709, Train Recon Loss: 0.06054, Val Recon Loss: 0.04871\n",
            "Epoch: 18/80, Training Time:506.53, Trained Samples: 28709/28709, Train Recon Loss: 0.05986, Val Recon Loss: 0.05427\n",
            "Epoch: 19/80, Training Time:534.02, Trained Samples: 28709/28709, Train Recon Loss: 0.05882, Val Recon Loss: 0.05310\n",
            "Epoch: 20/80, Training Time:561.04, Trained Samples: 28709/28709, Train Recon Loss: 0.05924, Val Recon Loss: 0.04605\n",
            "Epoch: 21/80, Training Time:588.39, Trained Samples: 28709/28709, Train Recon Loss: 0.05613, Val Recon Loss: 0.04713\n",
            "Epoch: 22/80, Training Time:618.49, Trained Samples: 28709/28709, Train Recon Loss: 0.06131, Val Recon Loss: 0.04975\n",
            "Epoch    23: reducing learning rate of group 0 to 2.1094e-02.\n",
            "Epoch: 23/80, Training Time:645.11, Trained Samples: 28709/28709, Train Recon Loss: 0.05551, Val Recon Loss: 0.07592\n",
            "Epoch: 24/80, Training Time:671.83, Trained Samples: 28709/28709, Train Recon Loss: 0.05781, Val Recon Loss: 0.04880\n",
            "Epoch: 25/80, Training Time:698.64, Trained Samples: 28709/28709, Train Recon Loss: 0.06126, Val Recon Loss: 0.04976\n",
            "Epoch    26: reducing learning rate of group 0 to 1.5820e-02.\n",
            "Epoch: 26/80, Training Time:725.43, Trained Samples: 28709/28709, Train Recon Loss: 0.06299, Val Recon Loss: 0.05083\n",
            "Epoch: 27/80, Training Time:752.00, Trained Samples: 28709/28709, Train Recon Loss: 0.05616, Val Recon Loss: 0.04728\n",
            "Epoch: 28/80, Training Time:778.64, Trained Samples: 28709/28709, Train Recon Loss: 0.07053, Val Recon Loss: 0.07688\n",
            "Epoch    29: reducing learning rate of group 0 to 1.1865e-02.\n",
            "Epoch: 29/80, Training Time:805.21, Trained Samples: 28709/28709, Train Recon Loss: 0.06929, Val Recon Loss: 0.05692\n",
            "Epoch: 30/80, Training Time:831.87, Trained Samples: 28709/28709, Train Recon Loss: 0.06278, Val Recon Loss: 0.06354\n",
            "Epoch: 31/80, Training Time:859.46, Trained Samples: 28709/28709, Train Recon Loss: 0.06914, Val Recon Loss: 0.06074\n",
            "Epoch    32: reducing learning rate of group 0 to 8.8989e-03.\n",
            "Epoch: 32/80, Training Time:889.21, Trained Samples: 28709/28709, Train Recon Loss: 0.06464, Val Recon Loss: 0.05869\n",
            "Epoch: 33/80, Training Time:916.85, Trained Samples: 28709/28709, Train Recon Loss: 0.06184, Val Recon Loss: 0.05536\n",
            "Epoch: 34/80, Training Time:944.51, Trained Samples: 28709/28709, Train Recon Loss: 0.05829, Val Recon Loss: 0.04990\n",
            "Epoch    35: reducing learning rate of group 0 to 6.6742e-03.\n",
            "Epoch: 35/80, Training Time:972.03, Trained Samples: 28709/28709, Train Recon Loss: 0.05667, Val Recon Loss: 0.05176\n",
            "Epoch: 36/80, Training Time:999.81, Trained Samples: 28709/28709, Train Recon Loss: 0.05296, Val Recon Loss: 0.04426\n",
            "Epoch: 37/80, Training Time:1027.27, Trained Samples: 28709/28709, Train Recon Loss: 0.05175, Val Recon Loss: 0.05277\n",
            "Epoch: 38/80, Training Time:1054.92, Trained Samples: 28709/28709, Train Recon Loss: 0.05021, Val Recon Loss: 0.04587\n",
            "Epoch    39: reducing learning rate of group 0 to 5.0056e-03.\n",
            "Epoch: 39/80, Training Time:1081.90, Trained Samples: 28709/28709, Train Recon Loss: 0.04976, Val Recon Loss: 0.04570\n",
            "Epoch: 40/80, Training Time:1109.43, Trained Samples: 28709/28709, Train Recon Loss: 0.04910, Val Recon Loss: 0.04864\n",
            "Epoch: 41/80, Training Time:1136.55, Trained Samples: 28709/28709, Train Recon Loss: 0.04718, Val Recon Loss: 0.04059\n",
            "Epoch: 42/80, Training Time:1166.41, Trained Samples: 28709/28709, Train Recon Loss: 0.04639, Val Recon Loss: 0.04840\n",
            "Epoch: 43/80, Training Time:1193.18, Trained Samples: 28709/28709, Train Recon Loss: 0.04813, Val Recon Loss: 0.04209\n",
            "Epoch    44: reducing learning rate of group 0 to 3.7542e-03.\n",
            "Epoch: 44/80, Training Time:1220.34, Trained Samples: 28709/28709, Train Recon Loss: 0.04681, Val Recon Loss: 0.04111\n",
            "Epoch: 45/80, Training Time:1247.15, Trained Samples: 28709/28709, Train Recon Loss: 0.04551, Val Recon Loss: 0.04132\n",
            "Epoch: 46/80, Training Time:1274.51, Trained Samples: 28709/28709, Train Recon Loss: 0.04557, Val Recon Loss: 0.03890\n",
            "Epoch: 47/80, Training Time:1301.40, Trained Samples: 28709/28709, Train Recon Loss: 0.04476, Val Recon Loss: 0.04245\n",
            "Epoch: 48/80, Training Time:1328.30, Trained Samples: 28709/28709, Train Recon Loss: 0.04624, Val Recon Loss: 0.04001\n",
            "Epoch    49: reducing learning rate of group 0 to 2.8157e-03.\n",
            "Epoch: 49/80, Training Time:1355.14, Trained Samples: 28709/28709, Train Recon Loss: 0.04621, Val Recon Loss: 0.04290\n",
            "Epoch: 50/80, Training Time:1381.71, Trained Samples: 28709/28709, Train Recon Loss: 0.04516, Val Recon Loss: 0.04090\n",
            "Epoch: 51/80, Training Time:1408.49, Trained Samples: 28709/28709, Train Recon Loss: 0.04522, Val Recon Loss: 0.04163\n",
            "Epoch    52: reducing learning rate of group 0 to 2.1118e-03.\n",
            "Epoch: 52/80, Training Time:1438.69, Trained Samples: 28709/28709, Train Recon Loss: 0.04524, Val Recon Loss: 0.04098\n",
            "Epoch: 53/80, Training Time:1465.55, Trained Samples: 28709/28709, Train Recon Loss: 0.04487, Val Recon Loss: 0.04187\n",
            "Epoch: 54/80, Training Time:1492.49, Trained Samples: 28709/28709, Train Recon Loss: 0.04467, Val Recon Loss: 0.04043\n",
            "Epoch    55: reducing learning rate of group 0 to 1.5838e-03.\n",
            "Epoch: 55/80, Training Time:1519.65, Trained Samples: 28709/28709, Train Recon Loss: 0.04441, Val Recon Loss: 0.04151\n",
            "Epoch: 56/80, Training Time:1546.71, Trained Samples: 28709/28709, Train Recon Loss: 0.04492, Val Recon Loss: 0.04174\n",
            "Epoch: 57/80, Training Time:1573.65, Trained Samples: 28709/28709, Train Recon Loss: 0.04508, Val Recon Loss: 0.04171\n",
            "Epoch    58: reducing learning rate of group 0 to 1.1879e-03.\n",
            "Epoch: 58/80, Training Time:1600.50, Trained Samples: 28709/28709, Train Recon Loss: 0.04524, Val Recon Loss: 0.04151\n",
            "Epoch: 59/80, Training Time:1627.54, Trained Samples: 28709/28709, Train Recon Loss: 0.04457, Val Recon Loss: 0.04151\n",
            "Epoch: 60/80, Training Time:1654.33, Trained Samples: 28709/28709, Train Recon Loss: 0.04428, Val Recon Loss: 0.04071\n",
            "Epoch    61: reducing learning rate of group 0 to 8.9090e-04.\n",
            "Epoch: 61/80, Training Time:1681.43, Trained Samples: 28709/28709, Train Recon Loss: 0.04430, Val Recon Loss: 0.04153\n",
            "Epoch: 62/80, Training Time:1711.79, Trained Samples: 28709/28709, Train Recon Loss: 0.04436, Val Recon Loss: 0.04179\n",
            "Epoch: 63/80, Training Time:1739.13, Trained Samples: 28709/28709, Train Recon Loss: 0.04412, Val Recon Loss: 0.04129\n",
            "Epoch    64: reducing learning rate of group 0 to 6.6817e-04.\n",
            "Epoch: 64/80, Training Time:1766.56, Trained Samples: 28709/28709, Train Recon Loss: 0.04385, Val Recon Loss: 0.04149\n",
            "Epoch: 65/80, Training Time:1793.42, Trained Samples: 28709/28709, Train Recon Loss: 0.04370, Val Recon Loss: 0.04078\n",
            "Epoch: 66/80, Training Time:1820.58, Trained Samples: 28709/28709, Train Recon Loss: 0.04344, Val Recon Loss: 0.04053\n",
            "Epoch    67: reducing learning rate of group 0 to 5.0113e-04.\n",
            "Epoch: 67/80, Training Time:1848.02, Trained Samples: 28709/28709, Train Recon Loss: 0.04345, Val Recon Loss: 0.04071\n",
            "Epoch: 68/80, Training Time:1875.25, Trained Samples: 28709/28709, Train Recon Loss: 0.04328, Val Recon Loss: 0.04001\n",
            "Epoch: 69/80, Training Time:1902.20, Trained Samples: 28709/28709, Train Recon Loss: 0.04319, Val Recon Loss: 0.03992\n",
            "Epoch    70: reducing learning rate of group 0 to 3.7585e-04.\n",
            "Epoch: 70/80, Training Time:1929.12, Trained Samples: 28709/28709, Train Recon Loss: 0.04305, Val Recon Loss: 0.04013\n",
            "Epoch: 71/80, Training Time:1956.17, Trained Samples: 28709/28709, Train Recon Loss: 0.04299, Val Recon Loss: 0.03986\n",
            "Epoch: 72/80, Training Time:1986.44, Trained Samples: 28709/28709, Train Recon Loss: 0.04291, Val Recon Loss: 0.04024\n",
            "Epoch    73: reducing learning rate of group 0 to 2.8189e-04.\n",
            "Epoch: 73/80, Training Time:2012.98, Trained Samples: 28709/28709, Train Recon Loss: 0.04287, Val Recon Loss: 0.03975\n",
            "Epoch: 74/80, Training Time:2039.63, Trained Samples: 28709/28709, Train Recon Loss: 0.04276, Val Recon Loss: 0.04013\n",
            "Epoch: 75/80, Training Time:2066.41, Trained Samples: 28709/28709, Train Recon Loss: 0.04264, Val Recon Loss: 0.03965\n",
            "Epoch    76: reducing learning rate of group 0 to 2.1141e-04.\n",
            "Epoch: 76/80, Training Time:2093.31, Trained Samples: 28709/28709, Train Recon Loss: 0.04265, Val Recon Loss: 0.04020\n",
            "Epoch: 77/80, Training Time:2120.95, Trained Samples: 28709/28709, Train Recon Loss: 0.04261, Val Recon Loss: 0.03985\n",
            "Epoch: 78/80, Training Time:2149.10, Trained Samples: 28709/28709, Train Recon Loss: 0.04258, Val Recon Loss: 0.03976\n",
            "Epoch    79: reducing learning rate of group 0 to 1.5856e-04.\n",
            "Epoch: 79/80, Training Time:2176.76, Trained Samples: 28709/28709, Train Recon Loss: 0.04266, Val Recon Loss: 0.04020\n",
            "Epoch: 80/80, Training Time:2203.99, Trained Samples: 28709/28709, Train Recon Loss: 0.04241, Val Recon Loss: 0.03976\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80g2z2RCTO4G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "fbbc9550-a925-4d4d-9d42-8ac853a6ddda"
      },
      "source": [
        "make_plots(pre_train_loss, pre_val_loss, \"Autoencoder\", \"Loss\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "SN_TS10dopF3",
        "outputId": "189ff022-dfed-4ebc-929c-eb4b0ea7bc71"
      },
      "source": [
        "#compare the reconstructed image with the original image\n",
        "fixed_x = train_dataset[random.randint(1,100)][0].unsqueeze(0).to(device)\n",
        "compare_x = compare(fixed_x)\n",
        "\n",
        "save_image(compare_x.data.cpu(), 'sample_image.png')\n",
        "display(Image('sample_image.png', width=700, unconfined=True))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": [],
            "image/png": {
              "width": 700,
              "unconfined": true
            }
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LwYdn6i0EOf"
      },
      "source": [
        "class_criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.75, patience=2, verbose=True)\n",
        "epoch = 80"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "oiQMp3NBz9jl",
        "outputId": "4c429a53-2c22-4d47-a410-2a08ef579ea6"
      },
      "source": [
        "train_class_losses, train_accs, val_class_losses, val_accs = train_class(model, train_loader, dev_loader, class_criterion, optimizer, scheduler, epochs, device)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/80, Training Time:20.68, Trained Samples: 28709/28709, Train Class Loss: 4.54429 Train Accuracy: 0.24494, Val Class Loss: 1.87519, Val Accuracy: 0.24700\n",
            "Epoch: 2/80, Training Time:41.01, Trained Samples: 28709/28709, Train Class Loss: 1.85516 Train Accuracy: 0.25131, Val Class Loss: 1.84374, Val Accuracy: 0.24728\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-8f0f5f83adbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_class_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_class_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_criterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-37-e428d1a07466>\u001b[0m in \u001b[0;36mtrain_class\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs, device)\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0mclass_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0mclass_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    117\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m                    )\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wdsye1u0Gpi"
      },
      "source": [
        "make_plots(train_accs, val_accs, \"Classifier\", \"Accuracy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZwlOTOU0zDX"
      },
      "source": [
        "# Train and Test the Multitasking Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFoxCJBY0352",
        "outputId": "3a5b7840-828c-4dc2-8929-1f5c091698f6"
      },
      "source": [
        "model = AutoClassifier()\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AutoClassifier(\n",
              "  (encoder): Encoder(\n",
              "    (encoderLayer1): EncoderBlock(\n",
              "      (Conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act1): ReLU()\n",
              "      (Conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act2): ReLU()\n",
              "      (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "    (encoderLayer2): EncoderBlock(\n",
              "      (Conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act1): ReLU()\n",
              "      (Conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act2): ReLU()\n",
              "      (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "    (encoderLayer3): EncoderBlock(\n",
              "      (Conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act1): ReLU()\n",
              "      (Conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act2): ReLU()\n",
              "      (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "    (encoderLayer4): EncoderBlock(\n",
              "      (Conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act1): ReLU()\n",
              "      (Conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (act2): ReLU()\n",
              "      (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "    (encoderLin): Sequential(\n",
              "      (0): Flatten(start_dim=1, end_dim=-1)\n",
              "      (1): Linear(in_features=2304, out_features=1000, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (TConv1): ConvTranspose2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (act1): ReLU()\n",
              "    (pool1): MaxUnpool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
              "    (TConv2): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (act2): ReLU()\n",
              "    (pool2): MaxUnpool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
              "    (TConv3): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (act3): ReLU()\n",
              "    (TConv4): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (act4): ReLU()\n",
              "    (pool3): MaxUnpool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
              "    (TConv5): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (act5): ReLU()\n",
              "    (TConv6): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm6): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (act6): ReLU()\n",
              "    (pool4): MaxUnpool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
              "    (TConv7): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (act7): ReLU()\n",
              "    (TConv8): ConvTranspose2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (decoderLin): Sequential(\n",
              "      (0): Linear(in_features=1000, out_features=2304, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (classifier): LinClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): ReLU()\n",
              "      (1): Linear(in_features=1000, out_features=512, bias=True)\n",
              "      (2): ReLU()\n",
              "      (3): Dropout(p=0.5, inplace=False)\n",
              "      (4): Linear(in_features=512, out_features=7, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DI0bduR60353"
      },
      "source": [
        "recon_criterion = nn.MSELoss()\n",
        "class_criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.75, patience=2, verbose=True)\n",
        "epochs = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ej0XBJed0353",
        "outputId": "d6afa47f-758b-4f72-b810-9a3f1fcba3e9"
      },
      "source": [
        "train_recon_losses, train_class_losses, train_accs, val_class_losses, val_recon_losses, val_accs = train(model, train_loader, dev_loader, recon_criterion, class_criterion, optimizer, scheduler, epochs, device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/100, Training Time:22.08, Trained Samples: 28709/28709, Train Total Loss: 2.00723, Train Recon Loss: 0.19358, Train Class Loss: 1.81365 Train Accuracy: 0.24289, Val Total Loss: 1.86848, Val Recon Loss: 0.09256, Val Class Loss: 1.77592, Val Accuracy: 0.24868\n",
            "Epoch: 2/100, Training Time:44.35, Trained Samples: 28709/28709, Train Total Loss: 1.79219, Train Recon Loss: 0.10203, Train Class Loss: 1.69017 Train Accuracy: 0.31760, Val Total Loss: 1.66965, Val Recon Loss: 0.11322, Val Class Loss: 1.55643, Val Accuracy: 0.38528\n",
            "Epoch: 3/100, Training Time:66.73, Trained Samples: 28709/28709, Train Total Loss: 1.60358, Train Recon Loss: 0.09055, Train Class Loss: 1.51303 Train Accuracy: 0.40782, Val Total Loss: 1.52458, Val Recon Loss: 0.11369, Val Class Loss: 1.41089, Val Accuracy: 0.45163\n",
            "Epoch: 4/100, Training Time:88.70, Trained Samples: 28709/28709, Train Total Loss: 1.46478, Train Recon Loss: 0.08229, Train Class Loss: 1.38249 Train Accuracy: 0.46233, Val Total Loss: 1.34615, Val Recon Loss: 0.07582, Val Class Loss: 1.27032, Val Accuracy: 0.51073\n",
            "Epoch: 5/100, Training Time:110.90, Trained Samples: 28709/28709, Train Total Loss: 1.39625, Train Recon Loss: 0.07042, Train Class Loss: 1.32583 Train Accuracy: 0.49211, Val Total Loss: 1.33041, Val Recon Loss: 0.06294, Val Class Loss: 1.26747, Val Accuracy: 0.50795\n",
            "Epoch: 6/100, Training Time:132.98, Trained Samples: 28709/28709, Train Total Loss: 1.34121, Train Recon Loss: 0.06263, Train Class Loss: 1.27857 Train Accuracy: 0.50740, Val Total Loss: 1.25664, Val Recon Loss: 0.05722, Val Class Loss: 1.19942, Val Accuracy: 0.53582\n",
            "Epoch: 7/100, Training Time:155.18, Trained Samples: 28709/28709, Train Total Loss: 1.29898, Train Recon Loss: 0.05992, Train Class Loss: 1.23906 Train Accuracy: 0.52158, Val Total Loss: 1.22102, Val Recon Loss: 0.06040, Val Class Loss: 1.16063, Val Accuracy: 0.54530\n",
            "Epoch: 8/100, Training Time:177.59, Trained Samples: 28709/28709, Train Total Loss: 1.26443, Train Recon Loss: 0.05592, Train Class Loss: 1.20851 Train Accuracy: 0.53471, Val Total Loss: 1.21963, Val Recon Loss: 0.05047, Val Class Loss: 1.16916, Val Accuracy: 0.54948\n",
            "Epoch: 9/100, Training Time:200.31, Trained Samples: 28709/28709, Train Total Loss: 1.23690, Train Recon Loss: 0.05168, Train Class Loss: 1.18522 Train Accuracy: 0.54924, Val Total Loss: 1.16503, Val Recon Loss: 0.04843, Val Class Loss: 1.11660, Val Accuracy: 0.57151\n",
            "Epoch: 10/100, Training Time:222.89, Trained Samples: 28709/28709, Train Total Loss: 1.20763, Train Recon Loss: 0.04914, Train Class Loss: 1.15850 Train Accuracy: 0.56003, Val Total Loss: 1.18683, Val Recon Loss: 0.05166, Val Class Loss: 1.13516, Val Accuracy: 0.57708\n",
            "Epoch: 11/100, Training Time:245.81, Trained Samples: 28709/28709, Train Total Loss: 1.17920, Train Recon Loss: 0.04708, Train Class Loss: 1.13211 Train Accuracy: 0.57087, Val Total Loss: 1.13318, Val Recon Loss: 0.05990, Val Class Loss: 1.07328, Val Accuracy: 0.59325\n",
            "Epoch: 12/100, Training Time:267.90, Trained Samples: 28709/28709, Train Total Loss: 1.16165, Train Recon Loss: 0.04616, Train Class Loss: 1.11549 Train Accuracy: 0.57954, Val Total Loss: 1.10219, Val Recon Loss: 0.04052, Val Class Loss: 1.06167, Val Accuracy: 0.59716\n",
            "Epoch: 13/100, Training Time:289.89, Trained Samples: 28709/28709, Train Total Loss: 1.13626, Train Recon Loss: 0.04384, Train Class Loss: 1.09242 Train Accuracy: 0.58936, Val Total Loss: 1.11175, Val Recon Loss: 0.04674, Val Class Loss: 1.06501, Val Accuracy: 0.59409\n",
            "Epoch: 14/100, Training Time:312.36, Trained Samples: 28709/28709, Train Total Loss: 1.11587, Train Recon Loss: 0.04235, Train Class Loss: 1.07352 Train Accuracy: 0.59643, Val Total Loss: 1.13362, Val Recon Loss: 0.04571, Val Class Loss: 1.08791, Val Accuracy: 0.59493\n",
            "Epoch: 15/100, Training Time:334.22, Trained Samples: 28709/28709, Train Total Loss: 1.10356, Train Recon Loss: 0.04036, Train Class Loss: 1.06319 Train Accuracy: 0.59950, Val Total Loss: 1.10574, Val Recon Loss: 0.04702, Val Class Loss: 1.05872, Val Accuracy: 0.60719\n",
            "Epoch: 16/100, Training Time:356.54, Trained Samples: 28709/28709, Train Total Loss: 1.08962, Train Recon Loss: 0.04061, Train Class Loss: 1.04902 Train Accuracy: 0.60545, Val Total Loss: 1.04739, Val Recon Loss: 0.03712, Val Class Loss: 1.01026, Val Accuracy: 0.62587\n",
            "Epoch: 17/100, Training Time:378.59, Trained Samples: 28709/28709, Train Total Loss: 1.07275, Train Recon Loss: 0.03904, Train Class Loss: 1.03370 Train Accuracy: 0.61162, Val Total Loss: 1.07528, Val Recon Loss: 0.03768, Val Class Loss: 1.03760, Val Accuracy: 0.61165\n",
            "Epoch: 18/100, Training Time:400.59, Trained Samples: 28709/28709, Train Total Loss: 1.05463, Train Recon Loss: 0.03765, Train Class Loss: 1.01698 Train Accuracy: 0.61465, Val Total Loss: 1.04756, Val Recon Loss: 0.03465, Val Class Loss: 1.01291, Val Accuracy: 0.62197\n",
            "Epoch    19: reducing learning rate of group 0 to 7.5000e-04.\n",
            "Epoch: 19/100, Training Time:422.67, Trained Samples: 28709/28709, Train Total Loss: 1.04534, Train Recon Loss: 0.03720, Train Class Loss: 1.00814 Train Accuracy: 0.62113, Val Total Loss: 1.09337, Val Recon Loss: 0.04193, Val Class Loss: 1.05144, Val Accuracy: 0.60524\n",
            "Epoch: 20/100, Training Time:444.88, Trained Samples: 28709/28709, Train Total Loss: 1.01259, Train Recon Loss: 0.03404, Train Class Loss: 0.97855 Train Accuracy: 0.63520, Val Total Loss: 1.02075, Val Recon Loss: 0.03279, Val Class Loss: 0.98795, Val Accuracy: 0.63814\n",
            "Epoch: 21/100, Training Time:467.26, Trained Samples: 28709/28709, Train Total Loss: 0.98654, Train Recon Loss: 0.03372, Train Class Loss: 0.95283 Train Accuracy: 0.64050, Val Total Loss: 1.01660, Val Recon Loss: 0.03033, Val Class Loss: 0.98628, Val Accuracy: 0.64427\n",
            "Epoch: 22/100, Training Time:489.67, Trained Samples: 28709/28709, Train Total Loss: 0.97731, Train Recon Loss: 0.03314, Train Class Loss: 0.94416 Train Accuracy: 0.64576, Val Total Loss: 1.00815, Val Recon Loss: 0.03113, Val Class Loss: 0.97702, Val Accuracy: 0.63451\n",
            "Epoch: 23/100, Training Time:511.83, Trained Samples: 28709/28709, Train Total Loss: 0.96303, Train Recon Loss: 0.03323, Train Class Loss: 0.92980 Train Accuracy: 0.64980, Val Total Loss: 1.05753, Val Recon Loss: 0.04180, Val Class Loss: 1.01574, Val Accuracy: 0.62057\n",
            "Epoch: 24/100, Training Time:534.23, Trained Samples: 28709/28709, Train Total Loss: 0.95689, Train Recon Loss: 0.03202, Train Class Loss: 0.92488 Train Accuracy: 0.65596, Val Total Loss: 1.02534, Val Recon Loss: 0.02982, Val Class Loss: 0.99552, Val Accuracy: 0.63173\n",
            "Epoch: 25/100, Training Time:556.81, Trained Samples: 28709/28709, Train Total Loss: 0.94642, Train Recon Loss: 0.03131, Train Class Loss: 0.91510 Train Accuracy: 0.65906, Val Total Loss: 0.99937, Val Recon Loss: 0.02855, Val Class Loss: 0.97082, Val Accuracy: 0.64065\n",
            "Epoch: 26/100, Training Time:579.28, Trained Samples: 28709/28709, Train Total Loss: 0.93223, Train Recon Loss: 0.03179, Train Class Loss: 0.90044 Train Accuracy: 0.66401, Val Total Loss: 1.02795, Val Recon Loss: 0.02697, Val Class Loss: 1.00098, Val Accuracy: 0.64148\n",
            "Epoch: 27/100, Training Time:601.79, Trained Samples: 28709/28709, Train Total Loss: 0.91333, Train Recon Loss: 0.03102, Train Class Loss: 0.88232 Train Accuracy: 0.66791, Val Total Loss: 1.03762, Val Recon Loss: 0.03113, Val Class Loss: 1.00649, Val Accuracy: 0.63591\n",
            "Epoch    28: reducing learning rate of group 0 to 5.6250e-04.\n",
            "Epoch: 28/100, Training Time:624.58, Trained Samples: 28709/28709, Train Total Loss: 0.90814, Train Recon Loss: 0.03026, Train Class Loss: 0.87788 Train Accuracy: 0.67501, Val Total Loss: 1.02579, Val Recon Loss: 0.02996, Val Class Loss: 0.99583, Val Accuracy: 0.64204\n",
            "Epoch: 29/100, Training Time:647.28, Trained Samples: 28709/28709, Train Total Loss: 0.87684, Train Recon Loss: 0.02983, Train Class Loss: 0.84701 Train Accuracy: 0.68425, Val Total Loss: 0.99884, Val Recon Loss: 0.02714, Val Class Loss: 0.97170, Val Accuracy: 0.64957\n",
            "Epoch: 30/100, Training Time:670.09, Trained Samples: 28709/28709, Train Total Loss: 0.85399, Train Recon Loss: 0.02953, Train Class Loss: 0.82446 Train Accuracy: 0.68964, Val Total Loss: 1.03527, Val Recon Loss: 0.02718, Val Class Loss: 1.00809, Val Accuracy: 0.64204\n",
            "Epoch    31: reducing learning rate of group 0 to 4.2188e-04.\n",
            "Epoch: 31/100, Training Time:693.54, Trained Samples: 28709/28709, Train Total Loss: 0.84723, Train Recon Loss: 0.02911, Train Class Loss: 0.81812 Train Accuracy: 0.69619, Val Total Loss: 1.01171, Val Recon Loss: 0.02617, Val Class Loss: 0.98553, Val Accuracy: 0.65319\n",
            "Epoch: 32/100, Training Time:716.57, Trained Samples: 28709/28709, Train Total Loss: 0.81940, Train Recon Loss: 0.02833, Train Class Loss: 0.79107 Train Accuracy: 0.70528, Val Total Loss: 1.00221, Val Recon Loss: 0.02515, Val Class Loss: 0.97706, Val Accuracy: 0.65319\n",
            "Epoch: 33/100, Training Time:739.21, Trained Samples: 28709/28709, Train Total Loss: 0.80335, Train Recon Loss: 0.02769, Train Class Loss: 0.77566 Train Accuracy: 0.71288, Val Total Loss: 1.01103, Val Recon Loss: 0.02493, Val Class Loss: 0.98609, Val Accuracy: 0.66239\n",
            "Epoch: 34/100, Training Time:761.61, Trained Samples: 28709/28709, Train Total Loss: 0.79146, Train Recon Loss: 0.02760, Train Class Loss: 0.76386 Train Accuracy: 0.71511, Val Total Loss: 0.97386, Val Recon Loss: 0.02551, Val Class Loss: 0.94835, Val Accuracy: 0.66128\n",
            "Epoch: 35/100, Training Time:784.32, Trained Samples: 28709/28709, Train Total Loss: 0.78099, Train Recon Loss: 0.02795, Train Class Loss: 0.75304 Train Accuracy: 0.71988, Val Total Loss: 1.01001, Val Recon Loss: 0.02521, Val Class Loss: 0.98480, Val Accuracy: 0.65152\n",
            "Epoch: 36/100, Training Time:807.12, Trained Samples: 28709/28709, Train Total Loss: 0.77154, Train Recon Loss: 0.02712, Train Class Loss: 0.74442 Train Accuracy: 0.72155, Val Total Loss: 1.01228, Val Recon Loss: 0.02498, Val Class Loss: 0.98730, Val Accuracy: 0.66044\n",
            "Epoch    37: reducing learning rate of group 0 to 3.1641e-04.\n",
            "Epoch: 37/100, Training Time:829.54, Trained Samples: 28709/28709, Train Total Loss: 0.76210, Train Recon Loss: 0.02706, Train Class Loss: 0.73504 Train Accuracy: 0.72838, Val Total Loss: 1.00223, Val Recon Loss: 0.02452, Val Class Loss: 0.97771, Val Accuracy: 0.65793\n",
            "Epoch: 38/100, Training Time:852.20, Trained Samples: 28709/28709, Train Total Loss: 0.73605, Train Recon Loss: 0.02686, Train Class Loss: 0.70919 Train Accuracy: 0.73799, Val Total Loss: 1.00632, Val Recon Loss: 0.02364, Val Class Loss: 0.98268, Val Accuracy: 0.66295\n",
            "Epoch: 39/100, Training Time:875.15, Trained Samples: 28709/28709, Train Total Loss: 0.72277, Train Recon Loss: 0.02690, Train Class Loss: 0.69587 Train Accuracy: 0.74381, Val Total Loss: 1.02572, Val Recon Loss: 0.02307, Val Class Loss: 1.00265, Val Accuracy: 0.66183\n",
            "Epoch    40: reducing learning rate of group 0 to 2.3730e-04.\n",
            "Epoch: 40/100, Training Time:897.90, Trained Samples: 28709/28709, Train Total Loss: 0.71679, Train Recon Loss: 0.02665, Train Class Loss: 0.69013 Train Accuracy: 0.74437, Val Total Loss: 1.00525, Val Recon Loss: 0.02350, Val Class Loss: 0.98176, Val Accuracy: 0.66434\n",
            "Epoch: 41/100, Training Time:920.65, Trained Samples: 28709/28709, Train Total Loss: 0.69562, Train Recon Loss: 0.02558, Train Class Loss: 0.67004 Train Accuracy: 0.75471, Val Total Loss: 1.01998, Val Recon Loss: 0.02270, Val Class Loss: 0.99728, Val Accuracy: 0.66936\n",
            "Epoch: 42/100, Training Time:943.14, Trained Samples: 28709/28709, Train Total Loss: 0.67814, Train Recon Loss: 0.02559, Train Class Loss: 0.65255 Train Accuracy: 0.75966, Val Total Loss: 1.04653, Val Recon Loss: 0.02386, Val Class Loss: 1.02267, Val Accuracy: 0.66183\n",
            "Epoch    43: reducing learning rate of group 0 to 1.7798e-04.\n",
            "Epoch: 43/100, Training Time:965.57, Trained Samples: 28709/28709, Train Total Loss: 0.67184, Train Recon Loss: 0.02567, Train Class Loss: 0.64616 Train Accuracy: 0.75976, Val Total Loss: 1.04269, Val Recon Loss: 0.02412, Val Class Loss: 1.01857, Val Accuracy: 0.65710\n",
            "Epoch: 44/100, Training Time:987.95, Trained Samples: 28709/28709, Train Total Loss: 0.65861, Train Recon Loss: 0.02511, Train Class Loss: 0.63350 Train Accuracy: 0.76600, Val Total Loss: 1.04419, Val Recon Loss: 0.02556, Val Class Loss: 1.01863, Val Accuracy: 0.67131\n",
            "Epoch: 45/100, Training Time:1010.50, Trained Samples: 28709/28709, Train Total Loss: 0.64923, Train Recon Loss: 0.02515, Train Class Loss: 0.62408 Train Accuracy: 0.77234, Val Total Loss: 1.05598, Val Recon Loss: 0.02273, Val Class Loss: 1.03325, Val Accuracy: 0.66992\n",
            "Epoch    46: reducing learning rate of group 0 to 1.3348e-04.\n",
            "Epoch: 46/100, Training Time:1032.55, Trained Samples: 28709/28709, Train Total Loss: 0.64050, Train Recon Loss: 0.02505, Train Class Loss: 0.61546 Train Accuracy: 0.77418, Val Total Loss: 1.06074, Val Recon Loss: 0.02291, Val Class Loss: 1.03783, Val Accuracy: 0.66908\n",
            "Epoch: 47/100, Training Time:1054.85, Trained Samples: 28709/28709, Train Total Loss: 0.62665, Train Recon Loss: 0.02503, Train Class Loss: 0.60162 Train Accuracy: 0.77875, Val Total Loss: 1.05993, Val Recon Loss: 0.02281, Val Class Loss: 1.03712, Val Accuracy: 0.66128\n",
            "Epoch: 48/100, Training Time:1077.03, Trained Samples: 28709/28709, Train Total Loss: 0.62317, Train Recon Loss: 0.02455, Train Class Loss: 0.59862 Train Accuracy: 0.77916, Val Total Loss: 1.05465, Val Recon Loss: 0.02256, Val Class Loss: 1.03209, Val Accuracy: 0.67020\n",
            "Epoch    49: reducing learning rate of group 0 to 1.0011e-04.\n",
            "Epoch: 49/100, Training Time:1099.85, Trained Samples: 28709/28709, Train Total Loss: 0.61168, Train Recon Loss: 0.02432, Train Class Loss: 0.58736 Train Accuracy: 0.78456, Val Total Loss: 1.06180, Val Recon Loss: 0.02277, Val Class Loss: 1.03903, Val Accuracy: 0.66351\n",
            "Epoch: 50/100, Training Time:1122.27, Trained Samples: 28709/28709, Train Total Loss: 0.60113, Train Recon Loss: 0.02438, Train Class Loss: 0.57675 Train Accuracy: 0.78968, Val Total Loss: 1.07757, Val Recon Loss: 0.02266, Val Class Loss: 1.05492, Val Accuracy: 0.66574\n",
            "Epoch: 51/100, Training Time:1144.93, Trained Samples: 28709/28709, Train Total Loss: 0.59113, Train Recon Loss: 0.02397, Train Class Loss: 0.56716 Train Accuracy: 0.79376, Val Total Loss: 1.09044, Val Recon Loss: 0.02266, Val Class Loss: 1.06778, Val Accuracy: 0.66685\n",
            "Epoch    52: reducing learning rate of group 0 to 7.5085e-05.\n",
            "Epoch: 52/100, Training Time:1167.19, Trained Samples: 28709/28709, Train Total Loss: 0.59689, Train Recon Loss: 0.02453, Train Class Loss: 0.57235 Train Accuracy: 0.79003, Val Total Loss: 1.09489, Val Recon Loss: 0.02210, Val Class Loss: 1.07279, Val Accuracy: 0.66434\n",
            "Epoch: 53/100, Training Time:1189.14, Trained Samples: 28709/28709, Train Total Loss: 0.57879, Train Recon Loss: 0.02419, Train Class Loss: 0.55460 Train Accuracy: 0.79696, Val Total Loss: 1.10065, Val Recon Loss: 0.02247, Val Class Loss: 1.07818, Val Accuracy: 0.67745\n",
            "Epoch: 54/100, Training Time:1210.87, Trained Samples: 28709/28709, Train Total Loss: 0.57983, Train Recon Loss: 0.02414, Train Class Loss: 0.55569 Train Accuracy: 0.79693, Val Total Loss: 1.09980, Val Recon Loss: 0.02241, Val Class Loss: 1.07739, Val Accuracy: 0.67494\n",
            "Epoch    55: reducing learning rate of group 0 to 5.6314e-05.\n",
            "Epoch: 55/100, Training Time:1232.91, Trained Samples: 28709/28709, Train Total Loss: 0.57072, Train Recon Loss: 0.02379, Train Class Loss: 0.54693 Train Accuracy: 0.80205, Val Total Loss: 1.09762, Val Recon Loss: 0.02199, Val Class Loss: 1.07563, Val Accuracy: 0.66853\n",
            "Epoch: 56/100, Training Time:1255.31, Trained Samples: 28709/28709, Train Total Loss: 0.56999, Train Recon Loss: 0.02338, Train Class Loss: 0.54661 Train Accuracy: 0.80177, Val Total Loss: 1.09275, Val Recon Loss: 0.02188, Val Class Loss: 1.07086, Val Accuracy: 0.67131\n",
            "Epoch: 57/100, Training Time:1277.36, Trained Samples: 28709/28709, Train Total Loss: 0.56793, Train Recon Loss: 0.02387, Train Class Loss: 0.54405 Train Accuracy: 0.80097, Val Total Loss: 1.10654, Val Recon Loss: 0.02203, Val Class Loss: 1.08451, Val Accuracy: 0.66713\n",
            "Epoch    58: reducing learning rate of group 0 to 4.2235e-05.\n",
            "Epoch: 58/100, Training Time:1299.60, Trained Samples: 28709/28709, Train Total Loss: 0.56488, Train Recon Loss: 0.02375, Train Class Loss: 0.54113 Train Accuracy: 0.80191, Val Total Loss: 1.10467, Val Recon Loss: 0.02190, Val Class Loss: 1.08277, Val Accuracy: 0.66908\n",
            "Epoch: 59/100, Training Time:1322.08, Trained Samples: 28709/28709, Train Total Loss: 0.55883, Train Recon Loss: 0.02359, Train Class Loss: 0.53524 Train Accuracy: 0.80550, Val Total Loss: 1.10917, Val Recon Loss: 0.02150, Val Class Loss: 1.08767, Val Accuracy: 0.67215\n",
            "Epoch: 60/100, Training Time:1344.30, Trained Samples: 28709/28709, Train Total Loss: 0.55393, Train Recon Loss: 0.02348, Train Class Loss: 0.53045 Train Accuracy: 0.80793, Val Total Loss: 1.12173, Val Recon Loss: 0.02172, Val Class Loss: 1.10001, Val Accuracy: 0.66992\n",
            "Epoch    61: reducing learning rate of group 0 to 3.1676e-05.\n",
            "Epoch: 61/100, Training Time:1366.87, Trained Samples: 28709/28709, Train Total Loss: 0.55208, Train Recon Loss: 0.02347, Train Class Loss: 0.52861 Train Accuracy: 0.80696, Val Total Loss: 1.13347, Val Recon Loss: 0.02196, Val Class Loss: 1.11152, Val Accuracy: 0.66992\n",
            "Epoch: 62/100, Training Time:1389.53, Trained Samples: 28709/28709, Train Total Loss: 0.54355, Train Recon Loss: 0.02346, Train Class Loss: 0.52009 Train Accuracy: 0.80989, Val Total Loss: 1.13676, Val Recon Loss: 0.02168, Val Class Loss: 1.11509, Val Accuracy: 0.66769\n",
            "Epoch: 63/100, Training Time:1411.95, Trained Samples: 28709/28709, Train Total Loss: 0.54102, Train Recon Loss: 0.02359, Train Class Loss: 0.51742 Train Accuracy: 0.81121, Val Total Loss: 1.14112, Val Recon Loss: 0.02164, Val Class Loss: 1.11948, Val Accuracy: 0.66936\n",
            "Epoch    64: reducing learning rate of group 0 to 2.3757e-05.\n",
            "Epoch: 64/100, Training Time:1434.39, Trained Samples: 28709/28709, Train Total Loss: 0.54014, Train Recon Loss: 0.02344, Train Class Loss: 0.51670 Train Accuracy: 0.81260, Val Total Loss: 1.13770, Val Recon Loss: 0.02145, Val Class Loss: 1.11624, Val Accuracy: 0.66602\n",
            "Epoch: 65/100, Training Time:1456.87, Trained Samples: 28709/28709, Train Total Loss: 0.53932, Train Recon Loss: 0.02395, Train Class Loss: 0.51538 Train Accuracy: 0.81302, Val Total Loss: 1.13605, Val Recon Loss: 0.02182, Val Class Loss: 1.11423, Val Accuracy: 0.66825\n",
            "Epoch: 66/100, Training Time:1479.10, Trained Samples: 28709/28709, Train Total Loss: 0.53110, Train Recon Loss: 0.02349, Train Class Loss: 0.50761 Train Accuracy: 0.81563, Val Total Loss: 1.13735, Val Recon Loss: 0.02151, Val Class Loss: 1.11584, Val Accuracy: 0.66964\n",
            "Epoch    67: reducing learning rate of group 0 to 1.7818e-05.\n",
            "Epoch: 67/100, Training Time:1501.51, Trained Samples: 28709/28709, Train Total Loss: 0.54159, Train Recon Loss: 0.02323, Train Class Loss: 0.51836 Train Accuracy: 0.81187, Val Total Loss: 1.14022, Val Recon Loss: 0.02161, Val Class Loss: 1.11861, Val Accuracy: 0.66741\n",
            "Epoch: 68/100, Training Time:1523.98, Trained Samples: 28709/28709, Train Total Loss: 0.53416, Train Recon Loss: 0.02325, Train Class Loss: 0.51090 Train Accuracy: 0.81612, Val Total Loss: 1.13181, Val Recon Loss: 0.02169, Val Class Loss: 1.11012, Val Accuracy: 0.66908\n",
            "Epoch: 69/100, Training Time:1546.69, Trained Samples: 28709/28709, Train Total Loss: 0.52761, Train Recon Loss: 0.02314, Train Class Loss: 0.50447 Train Accuracy: 0.81800, Val Total Loss: 1.13878, Val Recon Loss: 0.02227, Val Class Loss: 1.11651, Val Accuracy: 0.66685\n",
            "Epoch    70: reducing learning rate of group 0 to 1.3363e-05.\n",
            "Epoch: 70/100, Training Time:1568.81, Trained Samples: 28709/28709, Train Total Loss: 0.53031, Train Recon Loss: 0.02357, Train Class Loss: 0.50674 Train Accuracy: 0.81494, Val Total Loss: 1.14421, Val Recon Loss: 0.02168, Val Class Loss: 1.12253, Val Accuracy: 0.66713\n",
            "Epoch: 71/100, Training Time:1591.59, Trained Samples: 28709/28709, Train Total Loss: 0.52901, Train Recon Loss: 0.02343, Train Class Loss: 0.50558 Train Accuracy: 0.81772, Val Total Loss: 1.14395, Val Recon Loss: 0.02151, Val Class Loss: 1.12245, Val Accuracy: 0.66685\n",
            "Epoch: 72/100, Training Time:1614.20, Trained Samples: 28709/28709, Train Total Loss: 0.52709, Train Recon Loss: 0.02330, Train Class Loss: 0.50379 Train Accuracy: 0.81567, Val Total Loss: 1.14542, Val Recon Loss: 0.02181, Val Class Loss: 1.12360, Val Accuracy: 0.66880\n",
            "Epoch    73: reducing learning rate of group 0 to 1.0023e-05.\n",
            "Epoch: 73/100, Training Time:1636.64, Trained Samples: 28709/28709, Train Total Loss: 0.52812, Train Recon Loss: 0.02354, Train Class Loss: 0.50458 Train Accuracy: 0.81448, Val Total Loss: 1.15256, Val Recon Loss: 0.02160, Val Class Loss: 1.13095, Val Accuracy: 0.66713\n",
            "Epoch: 74/100, Training Time:1659.02, Trained Samples: 28709/28709, Train Total Loss: 0.52789, Train Recon Loss: 0.02301, Train Class Loss: 0.50488 Train Accuracy: 0.81528, Val Total Loss: 1.14764, Val Recon Loss: 0.02153, Val Class Loss: 1.12611, Val Accuracy: 0.66853\n",
            "Epoch: 75/100, Training Time:1681.60, Trained Samples: 28709/28709, Train Total Loss: 0.52250, Train Recon Loss: 0.02325, Train Class Loss: 0.49925 Train Accuracy: 0.81797, Val Total Loss: 1.15200, Val Recon Loss: 0.02151, Val Class Loss: 1.13049, Val Accuracy: 0.66825\n",
            "Epoch    76: reducing learning rate of group 0 to 7.5169e-06.\n",
            "Epoch: 76/100, Training Time:1703.72, Trained Samples: 28709/28709, Train Total Loss: 0.52138, Train Recon Loss: 0.02327, Train Class Loss: 0.49811 Train Accuracy: 0.81765, Val Total Loss: 1.15442, Val Recon Loss: 0.02132, Val Class Loss: 1.13310, Val Accuracy: 0.66769\n",
            "Epoch: 77/100, Training Time:1726.29, Trained Samples: 28709/28709, Train Total Loss: 0.52208, Train Recon Loss: 0.02286, Train Class Loss: 0.49921 Train Accuracy: 0.82034, Val Total Loss: 1.14630, Val Recon Loss: 0.02287, Val Class Loss: 1.12343, Val Accuracy: 0.66908\n",
            "Epoch: 78/100, Training Time:1748.82, Trained Samples: 28709/28709, Train Total Loss: 0.51848, Train Recon Loss: 0.02306, Train Class Loss: 0.49542 Train Accuracy: 0.81992, Val Total Loss: 1.15034, Val Recon Loss: 0.02145, Val Class Loss: 1.12889, Val Accuracy: 0.66685\n",
            "Epoch    79: reducing learning rate of group 0 to 5.6377e-06.\n",
            "Epoch: 79/100, Training Time:1771.01, Trained Samples: 28709/28709, Train Total Loss: 0.52300, Train Recon Loss: 0.02318, Train Class Loss: 0.49983 Train Accuracy: 0.81856, Val Total Loss: 1.15340, Val Recon Loss: 0.02139, Val Class Loss: 1.13201, Val Accuracy: 0.66964\n",
            "Epoch: 80/100, Training Time:1793.34, Trained Samples: 28709/28709, Train Total Loss: 0.52754, Train Recon Loss: 0.02314, Train Class Loss: 0.50441 Train Accuracy: 0.81772, Val Total Loss: 1.14877, Val Recon Loss: 0.02151, Val Class Loss: 1.12726, Val Accuracy: 0.66602\n",
            "Epoch: 81/100, Training Time:1815.67, Trained Samples: 28709/28709, Train Total Loss: 0.52029, Train Recon Loss: 0.02319, Train Class Loss: 0.49710 Train Accuracy: 0.81967, Val Total Loss: 1.15440, Val Recon Loss: 0.02159, Val Class Loss: 1.13281, Val Accuracy: 0.66853\n",
            "Epoch    82: reducing learning rate of group 0 to 4.2283e-06.\n",
            "Epoch: 82/100, Training Time:1837.71, Trained Samples: 28709/28709, Train Total Loss: 0.52014, Train Recon Loss: 0.02303, Train Class Loss: 0.49710 Train Accuracy: 0.82013, Val Total Loss: 1.15101, Val Recon Loss: 0.02207, Val Class Loss: 1.12894, Val Accuracy: 0.66685\n",
            "Epoch: 83/100, Training Time:1859.34, Trained Samples: 28709/28709, Train Total Loss: 0.52315, Train Recon Loss: 0.02307, Train Class Loss: 0.50008 Train Accuracy: 0.81960, Val Total Loss: 1.15040, Val Recon Loss: 0.02157, Val Class Loss: 1.12883, Val Accuracy: 0.66741\n",
            "Epoch: 84/100, Training Time:1881.90, Trained Samples: 28709/28709, Train Total Loss: 0.52303, Train Recon Loss: 0.02336, Train Class Loss: 0.49967 Train Accuracy: 0.81891, Val Total Loss: 1.14952, Val Recon Loss: 0.02249, Val Class Loss: 1.12702, Val Accuracy: 0.66880\n",
            "Epoch    85: reducing learning rate of group 0 to 3.1712e-06.\n",
            "Epoch: 85/100, Training Time:1903.49, Trained Samples: 28709/28709, Train Total Loss: 0.51844, Train Recon Loss: 0.02318, Train Class Loss: 0.49526 Train Accuracy: 0.81967, Val Total Loss: 1.15603, Val Recon Loss: 0.02200, Val Class Loss: 1.13403, Val Accuracy: 0.66741\n",
            "Epoch: 86/100, Training Time:1925.04, Trained Samples: 28709/28709, Train Total Loss: 0.52552, Train Recon Loss: 0.02327, Train Class Loss: 0.50225 Train Accuracy: 0.81804, Val Total Loss: 1.15047, Val Recon Loss: 0.02152, Val Class Loss: 1.12895, Val Accuracy: 0.66936\n",
            "Epoch: 87/100, Training Time:1947.07, Trained Samples: 28709/28709, Train Total Loss: 0.51803, Train Recon Loss: 0.02289, Train Class Loss: 0.49514 Train Accuracy: 0.82037, Val Total Loss: 1.15095, Val Recon Loss: 0.02167, Val Class Loss: 1.12928, Val Accuracy: 0.66685\n",
            "Epoch    88: reducing learning rate of group 0 to 2.3784e-06.\n",
            "Epoch: 88/100, Training Time:1968.84, Trained Samples: 28709/28709, Train Total Loss: 0.52159, Train Recon Loss: 0.02336, Train Class Loss: 0.49824 Train Accuracy: 0.81769, Val Total Loss: 1.15822, Val Recon Loss: 0.02165, Val Class Loss: 1.13657, Val Accuracy: 0.66769\n",
            "Epoch: 89/100, Training Time:1990.27, Trained Samples: 28709/28709, Train Total Loss: 0.51933, Train Recon Loss: 0.02290, Train Class Loss: 0.49643 Train Accuracy: 0.81866, Val Total Loss: 1.15847, Val Recon Loss: 0.02177, Val Class Loss: 1.13670, Val Accuracy: 0.66853\n",
            "Epoch: 90/100, Training Time:2012.04, Trained Samples: 28709/28709, Train Total Loss: 0.52171, Train Recon Loss: 0.02311, Train Class Loss: 0.49861 Train Accuracy: 0.82034, Val Total Loss: 1.15455, Val Recon Loss: 0.02139, Val Class Loss: 1.13316, Val Accuracy: 0.66685\n",
            "Epoch    91: reducing learning rate of group 0 to 1.7838e-06.\n",
            "Epoch: 91/100, Training Time:2034.28, Trained Samples: 28709/28709, Train Total Loss: 0.52220, Train Recon Loss: 0.02307, Train Class Loss: 0.49913 Train Accuracy: 0.81946, Val Total Loss: 1.15082, Val Recon Loss: 0.02161, Val Class Loss: 1.12921, Val Accuracy: 0.66741\n",
            "Epoch: 92/100, Training Time:2055.96, Trained Samples: 28709/28709, Train Total Loss: 0.51759, Train Recon Loss: 0.02305, Train Class Loss: 0.49454 Train Accuracy: 0.82197, Val Total Loss: 1.15437, Val Recon Loss: 0.02135, Val Class Loss: 1.13301, Val Accuracy: 0.66602\n",
            "Epoch: 93/100, Training Time:2078.14, Trained Samples: 28709/28709, Train Total Loss: 0.51801, Train Recon Loss: 0.02297, Train Class Loss: 0.49504 Train Accuracy: 0.81877, Val Total Loss: 1.16484, Val Recon Loss: 0.02164, Val Class Loss: 1.14320, Val Accuracy: 0.66936\n",
            "Epoch    94: reducing learning rate of group 0 to 1.3379e-06.\n",
            "Epoch: 94/100, Training Time:2100.75, Trained Samples: 28709/28709, Train Total Loss: 0.51968, Train Recon Loss: 0.02328, Train Class Loss: 0.49639 Train Accuracy: 0.81905, Val Total Loss: 1.15874, Val Recon Loss: 0.02147, Val Class Loss: 1.13728, Val Accuracy: 0.66741\n",
            "Epoch: 95/100, Training Time:2122.76, Trained Samples: 28709/28709, Train Total Loss: 0.51259, Train Recon Loss: 0.02331, Train Class Loss: 0.48927 Train Accuracy: 0.82187, Val Total Loss: 1.15041, Val Recon Loss: 0.02192, Val Class Loss: 1.12849, Val Accuracy: 0.66825\n",
            "Epoch: 96/100, Training Time:2144.71, Trained Samples: 28709/28709, Train Total Loss: 0.51927, Train Recon Loss: 0.02311, Train Class Loss: 0.49616 Train Accuracy: 0.81793, Val Total Loss: 1.16068, Val Recon Loss: 0.02164, Val Class Loss: 1.13903, Val Accuracy: 0.66741\n",
            "Epoch    97: reducing learning rate of group 0 to 1.0034e-06.\n",
            "Epoch: 97/100, Training Time:2166.53, Trained Samples: 28709/28709, Train Total Loss: 0.51932, Train Recon Loss: 0.02287, Train Class Loss: 0.49645 Train Accuracy: 0.81922, Val Total Loss: 1.16122, Val Recon Loss: 0.02189, Val Class Loss: 1.13933, Val Accuracy: 0.66685\n",
            "Epoch: 98/100, Training Time:2188.24, Trained Samples: 28709/28709, Train Total Loss: 0.51839, Train Recon Loss: 0.02298, Train Class Loss: 0.49541 Train Accuracy: 0.82037, Val Total Loss: 1.15982, Val Recon Loss: 0.02167, Val Class Loss: 1.13815, Val Accuracy: 0.66880\n",
            "Epoch: 99/100, Training Time:2210.00, Trained Samples: 28709/28709, Train Total Loss: 0.51617, Train Recon Loss: 0.02300, Train Class Loss: 0.49318 Train Accuracy: 0.82135, Val Total Loss: 1.15741, Val Recon Loss: 0.02260, Val Class Loss: 1.13480, Val Accuracy: 0.66853\n",
            "Epoch   100: reducing learning rate of group 0 to 7.5254e-07.\n",
            "Epoch: 100/100, Training Time:2232.04, Trained Samples: 28709/28709, Train Total Loss: 0.51524, Train Recon Loss: 0.02299, Train Class Loss: 0.49225 Train Accuracy: 0.82138, Val Total Loss: 1.15864, Val Recon Loss: 0.02150, Val Class Loss: 1.13714, Val Accuracy: 0.66936\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITGPG5zH09Lc"
      },
      "source": [
        "_, _, test_acc = validate(model, test_loader, recon_criterion, class_criterion, device)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}